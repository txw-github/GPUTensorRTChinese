# Windows ä¸­æ–‡ç”µè§†å‰§éŸ³é¢‘è½¬æ–‡å­—ç³»ç»Ÿ - å®Œæ•´éƒ¨ç½²æŒ‡å—

## ç³»ç»Ÿè¦æ±‚

### ç¡¬ä»¶è¦æ±‚
- **æ˜¾å¡**: NVIDIA RTX 3060 Ti (6GB VRAM) æˆ–æ›´é«˜
- **å†…å­˜**: 16GB RAM æ¨è
- **å­˜å‚¨**: è‡³å°‘ 20GB å¯ç”¨ç©ºé—´
- **CPU**: Intel i5-8400 æˆ– AMD Ryzen 5 2600 ä»¥ä¸Š

### è½¯ä»¶è¦æ±‚
- **æ“ä½œç³»ç»Ÿ**: Windows 10/11 (64ä½)
- **Python**: 3.8 - 3.11 (æ¨è 3.10)
- **CUDA**: 11.8 æˆ– 12.1
- **FFmpeg**: æœ€æ–°ç‰ˆæœ¬

## ç¬¬ä¸€æ­¥ï¼šå®‰è£…åŸºç¡€ç¯å¢ƒ

### 1.1 å®‰è£… Python
1. è®¿é—® [Pythonå®˜ç½‘](https://www.python.org/downloads/windows/)
2. ä¸‹è½½ Python 3.10.x ç‰ˆæœ¬
3. **é‡è¦**: å®‰è£…æ—¶å‹¾é€‰ "Add Python to PATH"
4. éªŒè¯å®‰è£…ï¼š
```cmd
python --version
pip --version
```

### 1.2 å®‰è£… CUDA Toolkit
1. è®¿é—® [NVIDIA CUDAä¸‹è½½é¡µé¢](https://developer.nvidia.com/cuda-downloads)
2. é€‰æ‹©ï¼šWindows > x86_64 > ç‰ˆæœ¬ > exe(local)
3. ä¸‹è½½å¹¶å®‰è£… CUDA Toolkit 11.8 æˆ– 12.1
4. éªŒè¯å®‰è£…ï¼š
```cmd
nvcc --version
nvidia-smi
```

### 1.3 å®‰è£… FFmpeg
1. è®¿é—® [FFmpegå®˜ç½‘](https://ffmpeg.org/download.html#build-windows)
2. ä¸‹è½½ Windows ç‰ˆæœ¬
3. è§£å‹åˆ° `C:\ffmpeg`
4. æ·»åŠ  `C:\ffmpeg\bin` åˆ°ç³»ç»Ÿç¯å¢ƒå˜é‡ PATH
5. éªŒè¯å®‰è£…ï¼š
```cmd
ffmpeg -version
```

## ç¬¬äºŒæ­¥ï¼šåˆ›å»ºé¡¹ç›®ç¯å¢ƒ

### 2.1 åˆ›å»ºé¡¹ç›®æ–‡ä»¶å¤¹
```cmd
mkdir C:\ChineseTranscriber
cd C:\ChineseTranscriber
```

### 2.2 åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
```cmd
python -m venv venv
venv\Scripts\activate
```

### 2.3 å‡çº§ pip
```cmd
python -m pip install --upgrade pip
```

## ç¬¬ä¸‰æ­¥ï¼šå®‰è£… PyTorch å’Œä¾èµ–

### 3.1 å®‰è£… PyTorch (CUDAç‰ˆæœ¬)
**å¯¹äº CUDA 11.8:**
```cmd
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

**å¯¹äº CUDA 12.1:**
```cmd
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### 3.2 éªŒè¯ PyTorch GPU æ”¯æŒ
åˆ›å»ºæµ‹è¯•æ–‡ä»¶ `test_gpu.py`ï¼š
```python
import torch
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
    print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
```

è¿è¡Œæµ‹è¯•ï¼š
```cmd
python test_gpu.py
```

## ç¬¬å››æ­¥ï¼šå®‰è£…éŸ³é¢‘è½¬å½•ä¾èµ–

### 4.1 å®‰è£… Whisper
```cmd
pip install openai-whisper
```

### 4.2 å®‰è£…éŸ³é¢‘å¤„ç†åº“
```cmd
pip install librosa soundfile
```

### 4.3 å®‰è£…ä¸­æ–‡å¤„ç†åº“
```cmd
pip install jieba opencc-python-reimplemented
```

### 4.4 å®‰è£…å…¶ä»–ä¾èµ–
```cmd
pip install numpy scipy matplotlib
pip install fastapi uvicorn
pip install python-multipart
```

## ç¬¬äº”æ­¥ï¼šå®‰è£… TensorRT (å¯é€‰ï¼Œæå‡æ€§èƒ½)

### 5.1 ä¸‹è½½ TensorRT
1. è®¿é—® [NVIDIA TensorRTé¡µé¢](https://developer.nvidia.com/tensorrt)
2. æ³¨å†Œå¹¶ä¸‹è½½ TensorRT 8.6.x for Windows
3. è§£å‹åˆ° `C:\TensorRT`

### 5.2 å®‰è£… TensorRT Python åŒ…
```cmd
cd C:\TensorRT\python
pip install tensorrt-8.6.*.whl
```

### 5.3 è®¾ç½®ç¯å¢ƒå˜é‡
æ·»åŠ åˆ°ç³»ç»Ÿç¯å¢ƒå˜é‡ï¼š
- `TRT_LIBPATH`: `C:\TensorRT\lib`
- åœ¨ PATH ä¸­æ·»åŠ : `C:\TensorRT\lib`

## ç¬¬å…­æ­¥ï¼šä¸‹è½½å’Œé…ç½®é¡¹ç›®

### 6.1 åˆ›å»ºé¡¹ç›®ç»“æ„
```cmd
mkdir models
mkdir uploads
mkdir outputs
mkdir temp
```

### 6.2 åˆ›å»ºé…ç½®æ–‡ä»¶ `config.json`
```json
{
  "system": {
    "gpu_memory_fraction": 0.85,
    "max_concurrent_jobs": 1,
    "temp_dir": "./temp"
  },
  "models": {
    "default": "whisper-large-v3",
    "available": [
      {
        "name": "whisper-large-v3",
        "display_name": "Whisper Large V3 (æ¨è)",
        "memory_required": 4096,
        "download_url": "automatic"
      },
      {
        "name": "whisper-medium",
        "display_name": "Whisper Medium (å¹³è¡¡)",
        "memory_required": 2048,
        "download_url": "automatic"
      },
      {
        "name": "whisper-small",
        "display_name": "Whisper Small (å¿«é€Ÿ)",
        "memory_required": 1024,
        "download_url": "automatic"
      }
    ]
  },
  "chinese_processing": {
    "variant": "simplified",
    "multi_pronunciation": true,
    "smart_punctuation": true,
    "segmentation_method": "jieba"
  },
  "output": {
    "formats": ["srt", "vtt", "txt"],
    "encoding": "utf-8"
  }
}
```

## ç¬¬ä¸ƒæ­¥ï¼šåˆ›å»ºå¯åŠ¨è„šæœ¬

### 7.1 åˆ›å»º `start.bat`
```batch
@echo off
cd /d C:\ChineseTranscriber
call venv\Scripts\activate
echo æ­£åœ¨å¯åŠ¨ä¸­æ–‡ç”µè§†å‰§è½¬å½•ç³»ç»Ÿ...
echo è¯·åœ¨æµè§ˆå™¨ä¸­è®¿é—®: http://localhost:5000
python app.py
pause
```

### 7.2 åˆ›å»º `install_models.py`
```python
"""
æ¨¡å‹ä¸‹è½½å’Œå®‰è£…è„šæœ¬
"""
import whisper
import os
import json

def download_whisper_models():
    """ä¸‹è½½Whisperæ¨¡å‹"""
    models_to_download = ["small", "medium", "large-v3"]
    
    print("å¼€å§‹ä¸‹è½½Whisperæ¨¡å‹...")
    for model_name in models_to_download:
        try:
            print(f"ä¸‹è½½ {model_name} æ¨¡å‹...")
            model = whisper.load_model(model_name)
            print(f"âœ“ {model_name} æ¨¡å‹ä¸‹è½½å®Œæˆ")
        except Exception as e:
            print(f"âœ— {model_name} æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")

if __name__ == "__main__":
    download_whisper_models()
    print("æ¨¡å‹ä¸‹è½½å®Œæˆï¼")
```

## ç¬¬å…«æ­¥ï¼šåˆ›å»ºä¸»åº”ç”¨ç¨‹åº

### 8.1 åˆ›å»º `app.py`
```python
"""
ä¸­æ–‡ç”µè§†å‰§éŸ³é¢‘è½¬æ–‡å­—ç³»ç»Ÿ - ä¸»åº”ç”¨
"""
import os
import json
import asyncio
from fastapi import FastAPI, File, UploadFile, Form, HTTPException
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

# å¯¼å…¥æˆ‘ä»¬çš„è½¬å½•å™¨
from multi_model_transcriber import MultiModelTranscriber, TranscriptionConfig

app = FastAPI(title="ä¸­æ–‡ç”µè§†å‰§è½¬å½•ç³»ç»Ÿ")

# CORSè®¾ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€è½¬å½•å™¨
transcriber = MultiModelTranscriber()

@app.get("/", response_class=HTMLResponse)
async def read_root():
    """ä¸»é¡µ"""
    html_content = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>ä¸­æ–‡ç”µè§†å‰§è½¬å½•ç³»ç»Ÿ</title>
        <meta charset="UTF-8">
        <style>
            body { font-family: Arial, sans-serif; margin: 40px; }
            .container { max-width: 800px; margin: 0 auto; }
            .upload-area { border: 2px dashed #ccc; padding: 20px; text-align: center; }
            .button { background: #007cba; color: white; padding: 10px 20px; border: none; cursor: pointer; }
            .result { margin-top: 20px; padding: 20px; background: #f5f5f5; }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>ğŸ¬ ä¸­æ–‡ç”µè§†å‰§éŸ³é¢‘è½¬æ–‡å­—ç³»ç»Ÿ</h1>
            <p>æ”¯æŒå¤šç§AIæ¨¡å‹ï¼Œä¸“ä¸ºNVIDIA RTX 3060 Tiä¼˜åŒ–</p>
            
            <form id="uploadForm" enctype="multipart/form-data">
                <div class="upload-area">
                    <input type="file" id="videoFile" name="file" accept="video/*,audio/*" required>
                    <p>é€‰æ‹©è§†é¢‘æˆ–éŸ³é¢‘æ–‡ä»¶</p>
                </div>
                
                <div style="margin: 20px 0;">
                    <label>é€‰æ‹©è½¬å½•æ¨¡å‹:</label>
                    <select id="model" name="model">
                        <option value="whisper-large-v3">Whisper Large V3 (æ¨è)</option>
                        <option value="whisper-medium">Whisper Medium (å¹³è¡¡)</option>
                        <option value="whisper-small">Whisper Small (å¿«é€Ÿ)</option>
                    </select>
                </div>
                
                <button type="submit" class="button">å¼€å§‹è½¬å½•</button>
            </form>
            
            <div id="progress" style="display:none;">
                <h3>è½¬å½•è¿›åº¦:</h3>
                <div id="progressBar"></div>
                <div id="progressText"></div>
            </div>
            
            <div id="result" class="result" style="display:none;">
                <h3>è½¬å½•ç»“æœ:</h3>
                <div id="resultContent"></div>
            </div>
        </div>
        
        <script>
            document.getElementById('uploadForm').onsubmit = async (e) => {
                e.preventDefault();
                
                const formData = new FormData();
                const fileInput = document.getElementById('videoFile');
                const modelSelect = document.getElementById('model');
                
                formData.append('file', fileInput.files[0]);
                formData.append('model', modelSelect.value);
                
                document.getElementById('progress').style.display = 'block';
                document.getElementById('result').style.display = 'none';
                
                try {
                    const response = await fetch('/transcribe', {
                        method: 'POST',
                        body: formData
                    });
                    
                    const result = await response.json();
                    
                    if (result.success) {
                        document.getElementById('result').style.display = 'block';
                        document.getElementById('resultContent').innerHTML = 
                            '<h4>è½¬å½•æ–‡æœ¬:</h4>' +
                            '<pre>' + result.text + '</pre>' +
                            '<p>å¤„ç†æ—¶é—´: ' + result.processing_time + 'ç§’</p>' +
                            '<p>ä½¿ç”¨æ¨¡å‹: ' + result.model + '</p>';
                    } else {
                        alert('è½¬å½•å¤±è´¥: ' + result.error);
                    }
                } catch (error) {
                    alert('è¯·æ±‚å¤±è´¥: ' + error);
                } finally {
                    document.getElementById('progress').style.display = 'none';
                }
            }
        </script>
    </body>
    </html>
    """
    return html_content

@app.get("/models")
async def get_available_models():
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    try:
        models = transcriber.get_available_models()
        gpu_info = transcriber.gpu_info
        
        return {
            "models": models,
            "gpu_info": gpu_info,
            "system_ready": gpu_info["available"]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/transcribe")
async def transcribe_file(
    file: UploadFile = File(...),
    model: str = Form("whisper-large-v3"),
    language: str = Form("zh")
):
    """è½¬å½•æ–‡ä»¶"""
    try:
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        upload_path = f"uploads/{file.filename}"
        os.makedirs("uploads", exist_ok=True)
        
        with open(upload_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        
        # é…ç½®è½¬å½•å‚æ•°
        config = TranscriptionConfig(
            model_name=model,
            language=language,
            use_gpu=True,
            use_tensorrt=True
        )
        
        # æ‰§è¡Œè½¬å½•
        result = transcriber.transcribe_audio(upload_path, config)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.remove(upload_path)
        
        return {
            "success": True,
            "text": result.full_text,
            "segments": [
                {
                    "start": seg.start,
                    "end": seg.end,
                    "text": seg.text,
                    "confidence": seg.confidence
                }
                for seg in result.segments
            ],
            "model": result.model_used,
            "processing_time": round(result.processing_time, 2),
            "gpu_used": result.gpu_used,
            "tensorrt_used": result.tensorrt_used
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

if __name__ == "__main__":
    print("ğŸ¬ ä¸­æ–‡ç”µè§†å‰§è½¬å½•ç³»ç»Ÿå¯åŠ¨ä¸­...")
    print("è¯·åœ¨æµè§ˆå™¨ä¸­è®¿é—®: http://localhost:5000")
    
    uvicorn.run(app, host="0.0.0.0", port=5000)
```

## ç¬¬ä¹æ­¥ï¼šç³»ç»Ÿæµ‹è¯•å’ŒéªŒè¯

### 9.1 åˆ›å»ºæµ‹è¯•è„šæœ¬ `test_system.py`
```python
"""
ç³»ç»Ÿæµ‹è¯•è„šæœ¬
"""
import torch
from multi_model_transcriber import MultiModelTranscriber, GPUDetector

def test_gpu_setup():
    """æµ‹è¯•GPUè®¾ç½®"""
    print("=== GPUæµ‹è¯• ===")
    detector = GPUDetector()
    gpu_info = detector.detect_gpu()
    
    for key, value in gpu_info.items():
        print(f"{key}: {value}")
    
    if gpu_info["available"]:
        print("âœ“ GPUè®¾ç½®æ­£ç¡®")
        return True
    else:
        print("âœ— GPUè®¾ç½®æœ‰é—®é¢˜")
        return False

def test_models():
    """æµ‹è¯•æ¨¡å‹å¯ç”¨æ€§"""
    print("\n=== æ¨¡å‹æµ‹è¯• ===")
    transcriber = MultiModelTranscriber()
    models = transcriber.get_available_models()
    
    for model in models:
        compatibility = transcriber.check_model_compatibility(model["name"])
        status = "âœ“" if compatibility["compatible"] else "âœ—"
        print(f"{status} {model['display_name']}: {compatibility['reason']}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ä¸­æ–‡ç”µè§†å‰§è½¬å½•ç³»ç»Ÿ - ç³»ç»Ÿæµ‹è¯•")
    print("=" * 50)
    
    gpu_ok = test_gpu_setup()
    test_models()
    
    print("\n=== æµ‹è¯•æ€»ç»“ ===")
    if gpu_ok:
        print("âœ“ ç³»ç»Ÿå‡†å¤‡å°±ç»ªï¼Œå¯ä»¥å¼€å§‹ä½¿ç”¨ï¼")
    else:
        print("âœ— è¯·æ£€æŸ¥GPUå’ŒCUDAå®‰è£…")

if __name__ == "__main__":
    main()
```

## ç¬¬åæ­¥ï¼šå¯åŠ¨å’Œä½¿ç”¨

### 10.1 é¦–æ¬¡è¿è¡Œè®¾ç½®
1. æ‰“å¼€å‘½ä»¤æç¤ºç¬¦ï¼ˆç®¡ç†å‘˜æƒé™ï¼‰
2. è¿›å…¥é¡¹ç›®ç›®å½•ï¼š
```cmd
cd C:\ChineseTranscriber
```

3. æ¿€æ´»è™šæ‹Ÿç¯å¢ƒï¼š
```cmd
venv\Scripts\activate
```

4. ä¸‹è½½æ¨¡å‹ï¼š
```cmd
python install_models.py
```

5. æµ‹è¯•ç³»ç»Ÿï¼š
```cmd
python test_system.py
```

### 10.2 å¯åŠ¨åº”ç”¨
```cmd
python app.py
```

æˆ–è€…åŒå‡» `start.bat` æ–‡ä»¶

### 10.3 ä½¿ç”¨ç³»ç»Ÿ
1. åœ¨æµè§ˆå™¨ä¸­è®¿é—® `http://localhost:5000`
2. é€‰æ‹©è§†é¢‘æˆ–éŸ³é¢‘æ–‡ä»¶
3. é€‰æ‹©è½¬å½•æ¨¡å‹
4. ç‚¹å‡»"å¼€å§‹è½¬å½•"
5. ç­‰å¾…å¤„ç†å®Œæˆï¼ŒæŸ¥çœ‹ç»“æœ

## æ•…éšœæ’é™¤

### GPUç›¸å…³é—®é¢˜
1. **CUDAæœªæ£€æµ‹åˆ°**:
   - ç¡®è®¤å·²å®‰è£…NVIDIAé©±åŠ¨ç¨‹åº
   - é‡æ–°å®‰è£…CUDA Toolkit
   - é‡å¯è®¡ç®—æœº

2. **æ˜¾å­˜ä¸è¶³**:
   - å…³é—­å…¶ä»–GPUåº”ç”¨ç¨‹åº
   - é€‰æ‹©è¾ƒå°çš„æ¨¡å‹ï¼ˆå¦‚whisper-smallï¼‰
   - è°ƒæ•´é…ç½®ä¸­çš„gpu_memory_fraction

3. **æ¨¡å‹åŠ è½½å¤±è´¥**:
   - æ£€æŸ¥ç½‘ç»œè¿æ¥
   - æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹ï¼š`python install_models.py`

### éŸ³é¢‘å¤„ç†é—®é¢˜
1. **FFmpegé”™è¯¯**:
   - ç¡®è®¤FFmpegå·²æ­£ç¡®å®‰è£…
   - æ£€æŸ¥PATHç¯å¢ƒå˜é‡
   - é‡æ–°ä¸‹è½½FFmpeg

2. **æ–‡ä»¶æ ¼å¼ä¸æ”¯æŒ**:
   - æ”¯æŒçš„æ ¼å¼ï¼šMP4, AVI, MKV, MP3, WAV, M4A
   - ä½¿ç”¨FFmpegè½¬æ¢æ ¼å¼

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### é’ˆå¯¹RTX 3060 Tiçš„ä¼˜åŒ–
1. **æ¨¡å‹é€‰æ‹©**:
   - æ—¥å¸¸ä½¿ç”¨ï¼šwhisper-medium
   - é«˜è´¨é‡ï¼šwhisper-large-v3
   - å¿«é€Ÿå¤„ç†ï¼šwhisper-small

2. **ç³»ç»Ÿè®¾ç½®**:
   - å…³é—­Windowsæ¸¸æˆæ¨¡å¼
   - è®¾ç½®é«˜æ€§èƒ½ç”µæºè®¡åˆ’
   - ç¡®ä¿å……è¶³çš„æ•£çƒ­

3. **æ‰¹é‡å¤„ç†**:
   - ä¸€æ¬¡å¤„ç†ä¸€ä¸ªæ–‡ä»¶
   - é¿å…åŒæ—¶è¿è¡Œå…¶ä»–GPUç¨‹åº

## æ›´æ–°å’Œç»´æŠ¤

### å®šæœŸç»´æŠ¤
1. æ›´æ–°NVIDIAé©±åŠ¨ç¨‹åº
2. æ›´æ–°PythonåŒ…ï¼š
```cmd
pip install --upgrade torch whisper
```

3. æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼š
```cmd
rmdir /s temp
mkdir temp
```

---

## æŠ€æœ¯æ”¯æŒ

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š
1. ç³»ç»Ÿé…ç½®ï¼ˆGPUå‹å·ã€é©±åŠ¨ç‰ˆæœ¬ï¼‰
2. é”™è¯¯ä¿¡æ¯æˆªå›¾
3. `test_system.py` çš„è¾“å‡ºç»“æœ

è¿™ä¸ªéƒ¨ç½²æŒ‡å—ä¸“é—¨é’ˆå¯¹Windowsç³»ç»Ÿå’ŒNVIDIA RTX 3060 Tiæ˜¾å¡ä¼˜åŒ–ï¼Œæä¾›äº†å®Œæ•´çš„å®‰è£…å’Œé…ç½®æ­¥éª¤ã€‚æŒ‰ç…§æ­¤æŒ‡å—æ“ä½œï¼Œå³ä½¿æ²¡æœ‰ç¼–ç¨‹ç»éªŒçš„ç”¨æˆ·ä¹Ÿèƒ½æˆåŠŸéƒ¨ç½²è¿™ä¸ªä¸­æ–‡ç”µè§†å‰§éŸ³é¢‘è½¬æ–‡å­—ç³»ç»Ÿã€‚